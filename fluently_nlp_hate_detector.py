# -*- coding: utf-8 -*-
"""Fluently_NLP_Hate_Detector.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T0MPx92SEiww-KK4qDl-qhtpHw8sbWJ6
"""

import pandas as pd
import ast
import matplotlib.pyplot as plt

# === STEP 1: Load Davidson Dataset ===
df_hate = pd.read_csv("labeled_data.csv")

# Remap class: 0 → 2 (hate), 1 → 1 (offensive), 2 → 0 (friendly)
df_hate['label'] = df_hate['class'].map({0: 2, 1: 1, 2: 0})
df_hate = df_hate[['tweet', 'label']]

# === STEP 2: Load DailyDialog Files ===
df_train = pd.read_csv("train.csv")
df_test = pd.read_csv("test.csv")
df_val = pd.read_csv("validation.csv")

# Extract friendly utterances
def extract_utterances_from_dialog_column(df):
    all_utterances = []
    for dialog_str in df['dialog'].dropna():
        try:
            utterances = ast.literal_eval(dialog_str)
            if isinstance(utterances, list):
                all_utterances.extend([utt.strip() for utt in utterances if len(utt.strip()) > 3])
        except Exception:
            continue
    return all_utterances

utterances = (
    extract_utterances_from_dialog_column(df_train) +
    extract_utterances_from_dialog_column(df_test) +
    extract_utterances_from_dialog_column(df_val)
)

df_pos = pd.DataFrame({
    'tweet': utterances,
    'label': 0  # Friendly
})

# === STEP 3: Combine and Balance to 4000 samples (≈1333/class) ===
df_all = pd.concat([df_hate, df_pos], ignore_index=True)

target_count = 1333

# Sanity check: Do we have enough samples per class?
label_counts = df_all['label'].value_counts()
for label in [0, 1, 2]:
    if label_counts[label] < target_count:
        raise ValueError(f" Not enough samples for label {label}: only {label_counts[label]} available.")

# Sample and balance
df_balanced = pd.concat([
    df_all[df_all['label'] == 0].sample(target_count, random_state=42),
    df_all[df_all['label'] == 1].sample(target_count, random_state=42),
    df_all[df_all['label'] == 2].sample(target_count, random_state=42)
], ignore_index=True)

# Shuffle
df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)

# === STEP 4: Visualize
df_balanced['label'].value_counts().plot(kind='bar')
plt.title("Balanced Dataset (≈4000 samples)")
plt.xlabel("Label")
plt.ylabel("Count")
plt.xticks(ticks=[0, 1, 2], labels=["Friendly", "Offensive", "Hate"], rotation=0)
plt.show()

# === STEP 5: Preview and Save
print(df_balanced.head())
df_balanced.to_csv("balanced_dataset.csv", index=False)

pip install transformers datasets scikit-learn torch

import pandas as pd

# Load the balanced data
df = pd.read_csv("balanced_dataset.csv")

# Optional: Rename columns just in case
df = df.rename(columns={'tweet': 'text', 'label': 'label'})

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize the data
texts = df["text"].tolist()
labels = df["label"].tolist()

encodings = tokenizer(
    texts,
    padding=True,
    truncation=True,
    max_length=128,
    return_tensors="pt"
)

import torch
from torch.utils.data import Dataset

class HateDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = torch.tensor(labels)

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        item = {k: v[idx] for k, v in self.encodings.items()}
        item['labels'] = self.labels[idx]
        return item

dataset = HateDataset(encodings, labels)

from torch.utils.data import random_split, DataLoader

train_size = int(0.8 * len(dataset))
val_size = len(dataset) - train_size

train_dataset, val_dataset = random_split(dataset, [train_size, val_size])
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=16)

from transformers import BertForSequenceClassification
from torch.optim import AdamW
import torch

model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3
)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

optimizer = AdamW(model.parameters(), lr=5e-5)

from tqdm import tqdm

model.train()
for epoch in range(10):
    loop = tqdm(train_loader, leave=True)
    for batch in loop:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        loop.set_description(f"Epoch {epoch}")
        loop.set_postfix(loss=loss.item())

from sklearn.metrics import classification_report

model.eval()
preds = []
true_labels = []

with torch.no_grad():
    for batch in val_loader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        logits = outputs.logits
        preds.extend(torch.argmax(logits, axis=1).cpu().numpy())
        true_labels.extend(batch["labels"].cpu().numpy())

print(classification_report(true_labels, preds, target_names=["Friendly", "Offensive", "Hate"]))

model.save_pretrained("hate_detector_model/")
tokenizer.save_pretrained("hate_detector_model/")

pip install openai-whisper

import whisper
whisper_model = whisper.load_model("base")

from transformers import BertTokenizer, BertForSequenceClassification
import torch

# Load model and tokenizer
model = BertForSequenceClassification.from_pretrained("hate_detector_model")
tokenizer = BertTokenizer.from_pretrained("hate_detector_model")
model.eval()

# Prediction function
def predict_label(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    with torch.no_grad():
        outputs = model(**inputs)
        pred = torch.argmax(outputs.logits, dim=1).item()
    return ["Friendly", "Offensive", "Hate"][pred]

audio_path = "102.m4a"

result = whisper_model.transcribe(audio_path, language="en")
text = result["text"]
label = predict_label(text)

print("Transcribed:", text)
print("Predicted Label:", label)

# Save the trained model state dictionary
torch.save(model.state_dict(), 'nlp_hate_detector_model.pth')

# Load the saved state dictionary
model_state_dict = torch.load('nlp_hate_detector_model.pth', map_location=torch.device('cpu')) # or 'cuda' if you trained on GPU and want to load on GPU

# Load the model structure first
loaded_model = BertForSequenceClassification.from_pretrained(
    "bert-base-uncased",
    num_labels=3
)

# Load the state dictionary into the model
loaded_model.load_state_dict(model_state_dict)

# Move the loaded model to the appropriate device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
loaded_model.to(device)

# Set the model to evaluation mode
loaded_model.eval()

print("Model loaded successfully from hate_detector_model.pth")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Generate confusion matrix
cm = confusion_matrix(true_labels, preds)

# Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=["Friendly", "Offensive", "Hate"])
disp.plot(cmap=plt.cm.Blues)
plt.title("Confusion Matrix")
plt.show()

from google.colab import files
files.download('nlp_hate_detector_model.pth')